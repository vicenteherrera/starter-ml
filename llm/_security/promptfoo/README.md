
# promptfoo

Promptfoo is an LLM vulnerability scanner and "LLM Red Teaming" tool.

This example contains a comparison using a couple of promptfoo evaluations against OpenAI GPT-3.5-turbo and GPT-4.

## Usage

```bash
# Run container to show report with evaluation already done
make container-run
# Then open a browser to: http://localhost:15500
```

## More information

* https://www.promptfoo.dev/docs/getting-started/
* https://www.promptfoo.dev/docs/guides/prevent-llm-hallucations/
* https://promptfoo.dev/docs/configuration/expected-outputs/model-graded/
* https://promptfoo.dev/docs/guides/factuality-eval/
* https://github.com/promptfoo/promptfoo/tree/main/examples/self-grading
* https://github.com/promptfoo/promptfoo/tree/main/examples/simple-cli
* [HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2305.11747)
* [Practical Steps to Reduce Hallucination and Improve Performance of Systems Built with Large Language Models](https://medium.com/@victor.dibia/practical-steps-to-reduce-hallucination-and-improve-performance-of-systems-built-with-large-5d2bcadeba61)
* [Asemantic Induction of Hallucinations in Large Language Models](https://medium.com/starschema-blog/asemantic-induction-of-hallucinations-in-large-language-models-c92ef5030714)
* [‘AI package hallucination’ can spread malicious code into developer environments](https://www.scmagazine.com/news/ai-package-hallucination-malicious-code-developer-environments)
* [AI hallucinates software packages and devs download them – even if potentially poisoned with malware](https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/)

